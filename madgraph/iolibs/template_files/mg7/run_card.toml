[run]
run_name = "run"
device = "cpu" # options: cpu, gpu

[beam]
e_cm = 13000.0
pdf = "NNPDF23_lo_as_0130_qed"
fixed_ren_scale = true
fixed_fact_scale = true
ren_scale = 91.188
fact_scale1 = 91.188
fact_scale2 = 91.188
# options: transverse_energy, transverse_mass, half_transverse_mass, partonic_energy
dynamical_scale_choice = "half_transverse_mass"

[generation]
events = 10000
max_overweight_fraction = 0.01
max_overweight_truncation = 0.01
start_batch_size = 1000
max_batch_size = 16000
survey_min_iters = 3
survey_max_iters = 3
survey_target_precision = 0.1

[vegas]
bins = 64
damping = 0.4
optimization_patience = 5
optimization_threshold = 0.95

[phasespace]
mode = "multichannel" #options: multichannel, flat, both
sde_strategy = "diagrams" #options: diagrams, denominators
decays = "all" # options: all, massive, none
t_channel = "propagator" # options: propagator, rambo, chili
flat_mode = "rambo" # options: propagator, rambo, chili
simplified_channel_count = 10
nu = 0.7

[cuts]
# possible groups: jet, bottom, lepton, missing, photon
# possible observables: pt, eta, dR, mass, sqrt_s
#     (mass is for pairs of particles from the same group, sqrt_s is for all outgoing particles)
# for all cuts, min or max can be specified
jet.pt.min = 20.0
jet.eta.max = 5.0
jet.dR.min = 0.4
sqrt_s.min = 0.0

[madnis]
enable = false

# normalizing flow parameters
flow_hidden_dim = 32
flow_layers = 3
flow_spline_bins = 10
flow_activation = "leaky_relu" # options: relu, leaky_relu, elu, gelu, sigmoid, softplus
flow_invert_spline = false

# discrete dimensions
discrete_hidden_dim = 64
discrete_layers = 3
discrete_activation = "leaky_relu" # options: relu, leaky_relu, elu, gelu, sigmoid, softplus

# channel weight network
cwnet_hidden_dim = 64
cwnet_layers = 3
cwnet_activation = "leaky_relu" # options: relu, leaky_relu, elu, gelu, sigmoid, softplus

# training parameters
loss = "stratified_variance" # options: stratified_variance, kl_divergence, rkl_divergence
train_batches = 1250
log_interval = 100
batch_size_offset = 512
batch_size_per_channel = 128
lr = 1e-3
lr_decay = 0.01
lr_max = 3e-3
lr_scheduler = "cosine" # options: exponential, inverse, onecycle, cosine, none
train_mcw = true
buffer_capacity = 0
minimum_buffer_size = 50000
buffered_steps = 0
uniform_channel_ratio = 0.1
integration_history_length = 1000
max_stored_channel_weights = 100
channel_dropping_threshold = 0.001
channel_dropping_interval = 300
drop_zero_integrands = true
batch_size_threshold = 0.5
channel_grouping_mode = "uniform" # options: none, uniform, learned
fixed_cwnet_fraction = 0.33
